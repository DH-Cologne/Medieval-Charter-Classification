# Implementierung eines Programms zur Klassifikation der Bestandteile mittelalterlicher Königs- und Kaiserurkunden in Java <!-- omit in toc -->
Technische Dokumentation zum Repository Medieval Charter Classification (https://github.com/AlinaOs/Medieval-Charter-Classification)

Autorin: Alina Ostrowski;
Stand: August 2019

Die vorliegende Technische Dokumentation dokumentiert das Programm des GitHub-Repositorys [AlinaOs/Medieval-Charter-Classification](https://github.com/AlinaOs/Medieval-Charter-Classification). Das Programm wurde im Sommer 2019 mit dem Ziel entwickelt, einen Ansatz zu bieten, mit dem mittelalterliche Urkunden automatisch in ihre Formularbestandteile aufgeteilt und diese richtig erkannt (klassifiziert) werden können. Das Repository enthält den Programmcode im Ordner **src**. Alle weiteren Ordner werden vom Programm während der Laufzeit benutzt, enthalten aus urheberrechtlichen Gründen jedoch keine oder nur Teile der benötigten Dateien (z.B. sind die Trainingsdaten nicht enthalten). In der Dokumentation wird erläutert, wie die Ordner im Normalfall benutzt werden. Im Ordner **documentation** befinden sich zu Anschauungszwecken ein Ordner mit Kopien der Evaluationsergebnisse, die sich bei der Entwicklung des Programms ergeben haben, sowie die vorliegende Dokumentation. Diese Dokumentation befindet sich auf dem Stand vom August 2019. Änderungen, die seit diesem Zeitpunkt am Programmcode vorgenommen worden sind, sind der **readme** des Repositorys zu entnehmen.

## Inhalt <!-- omit in toc -->
- [1. Die Datenbasis](#1-die-datenbasis)
- [2. Einlesen der Daten und Erstellung der Objektrepräsentationen der Urkunden](#2-einlesen-der-daten-und-erstellung-der-objektrepräsentationen-der-urkunden)
- [3. Präprozessierung](#3-präprozessierung)
- [4. Klassifikation](#4-klassifikation)
- [5.  Evaluation](#5-evaluation)
- [6. Anhang: Tabelle Evaluationsergebnisse](#6-anhang-tabelle-evaluationsergebnisse)
 [Anhang: Tabelle Evaluationsergebnisse](#6-anhang-tabelle-evaluationsergebnisse)

## 1. Die Datenbasis
Die für das Training des Programms benutzten Daten stammen aus dem virtuellen Urkundenarchiv [Monasterium](https://www.monasterium.net/mom/home), welches auf seiner Website Zugriff auf über 600.000 Urkunden bietet. Die Urkunden stammen aus Archiven und Sammlungen verschiedenster europäischer Länder (insbesondere Österreich, Deutschland, Italien und Ungarn) sowie aus der OCR-gestützten Aufbereitung bereits im Druck erschienener Urkundeneditionen in Kooperation mit Google. Das Online-Portal von Monasterium wurde ursprünglich in Kooperation mit dem Kölner Institut für Historisch-Kulturwissenschaftliche Informationsverarbeitung (HKI, jetzt Lehrstuhl des [Instituts für Digital Humanities](https://dh.phil-fak.uni-koeln.de/)) entwickelt und wird momentan durch das Cologne Center for eHumanities ([CCeH](https://cceh.uni-koeln.de/)) betreut.

Monasterium nutzt zur Speicherung und Nutzbarmachung das Datenformat XML. Der genaue Standard, an den die einliefernden Institutionen sich halten müssen, ist der von der Charters Encoding Initiative an der Ludwig-Maximilians-Universität in München entwickelte Standard [CEI](https://www.cei.lmu.de/), welcher wiederum an demjenigen der Text Encoding Initiative (TEI) orientiert ist. Der CEI-Standard bietet spezielle Tags zur äußeren und inneren Beschreibung der Urkunden sowie neben einem allgemeinen Tag für den Urkundenvolltext (`<cei:tenor>`) auch eigene Tags für die einzelnen diplomatischen Urkundenbestandteile wie Protokoll, Invocatio etc. Das ermöglicht einen schnellen und gezielten Zugriff auf die benötigten Daten sowie die einfache Erweiterung der einzelnen Dateien.

Von den zahlreichen Urkunden-Dokumenten aus Monasterium war lediglich ein Bruchteil für das Vorhaben relevant bzw. überhaupt nutzbar. Denn die Urkunden müssen dafür gewisse Anforderungen erfüllen, insbesondere, dass sie eine Volltexttranskription enthalten (viele Urkunden aus Monasterium enthalten keine Transkription, sondern lediglich die Beschreibung) und auf Latein verfasst sind. Weitere Voraussetzungen waren auch, dass das Datum der Urkunde zwischen 950 und 1399 liegen muss sowie dass sie eines der Wörter *rex*, *regina*, *imperator* oder *imperatrix* enthalten muss (da für das Projekt nur Königsurkunden von Relevanz sind).

Um die verwendbaren Urkunden herauszufiltern, habe ich zunächst ein Filter-Programm (**helpers.MomFilter**) geschrieben, welches alle Urkunden einliest, ihre Tauglichkeit prüft sowie jede Urkunde danach in einen „guten“ oder „schlechten“ Ordner kopiert. Ohne weitere Überprüfung von der Verwendung ausgeschlossen wurden auch sämtliche Urkunden derjenigen Sammlungen, die durch die Zusammenarbeit mit Google erschlossen worden sind, da die mit OCR generierten Texte zahlreiche Schreibfehler enthalten, die eine effektive linguistische Präprozessierung nahezu undenkbar machen. Nach Anwendung des Filter-Programms und weiterer händischer Überprüfung ergaben sich schlussendlich 89 Urkunden, die annotiert und für die Klassifikation verwendet wurden. Die annotierten Trainingsdaten sind im Begleitrepository unter https://github.com/AlinaOs/Structurally_Annotated_Medieval_Charters zu finden. Die Urkunden stammen ausschließlich von römisch-deutschen Königen und Kaisern von Otto I. bis Otto IV. (ca. 960 – 1214). Daraus folgt gleichsam, dass das Programm die besten Ergebnisse für Urkunden aus dieser Zeit liefert.

## 2. Einlesen der Daten und Erstellung der Objektrepräsentationen der Urkunden
Beim Starten des Programms (durch `DiplomaAnalyzerApp.java`) kann zunächst zwischen den beiden Möglichkeiten Evaluation oder Klassifikation gewählt werden. Im Folgenden soll zunächst der Programmablauf für die Klassifikation bisher nicht annotierter Daten vorgestellt werden.

Die ersten Schritte für eine Klassifikation sind das Einlesen der Trainingsdaten sowie nach der Eingabe eines geeigneten Pfades (bei der Auswahl 'STANDARD' wird der Default-Ordner **data/inputData** ausgewertet) das Einlesen der Klassifikationsurkunden durch die Suche nach cei.xml-Dateien im übergebenen Pfad und die Anlage von Urkunden-Objekten für diese Dateien. Das Programm stellt für die Speicherung (im Arbeitsspeicher) und Verarbeitung der Urkunden zwei Datenklassen, `Diploma` und `TrainingDiploma`, bereit, die beide basale Funktionen von der Superklasse `AbstractDiploma` erben. `Diploma` erweitert die Superklasse um die Möglichkeit, die Label-Informationen des Urkundenobjekts als XML abzuspeichern, wohingegen den Trainingsurkunden diese Möglichkeit fehlt. Bei der Erstellung einer der beiden nicht-abstrakten Klassen wird die übergebene XML-Datei zunächst mithilfe der Java-eigenen Klassen und Methoden zur Verarbeitung von XML geparst und so in ein durchsuchbares und manipulierbares `Document`-Objekt übertragen. Das eigentliche Parsing wird dabei durch die statische `ReaderWriter`-Klasse übernommen.

Den Urkundenklassen obliegt als nächster Schritt die Erstellung der Satzobjekte, welche die eigentlichen Klassifikationseinheiten darstellen. Diese Objekte können – analog zu den Urkundenobjekten, die eine der Satzklassen als generischen Typ besitzen müssen – als `Sentence` oder `TrainingSentence` deklariert werden. Auch diese Klassen erben von einer gemeinsamen Superklasse `AbstractSentence`. Während die eigentlichen Unterschiede der Satzklassen nicht groß sind, ist die Trennung in zwei unterschiedliche Klassen dennoch sinnvoll, da somit sichergestellt werden kann, dass Trainingssätze von ihrer Erstellung an bereits eine bekannte Klassen-Zuordnung besitzen (Feldvariablen `truePartLabel` und `trueParagraphLabel` in der Klasse `TrainingSentence`). Die entsprechenden Werte müssen bereits im Konstruktor übergeben werden. Somit ist die Verwendung des richtigen Datentyps für die Evaluation sowie das Training des probabilistischen Klassifikators gesichert, während mithilfe des Typs `AbstractSentence` immer noch sowohl `Sentence` als auch `TrainingSentence` den normalen Klassifikationsprozess durchlaufen können.

Aufgrund der unterschiedlichen generischen Typen (`Diploma` nutzt `Sentence`, `TrainingDiploma` hingegen `TrainingSentence`), unterscheiden sich `Diploma` und `TrainingDiploma` in der Implementierung der durch die abstrakte Superklasse geforderten Methode `createSentences()`, die für die Erstellung der Sätze zuständig ist. `TrainingDiploma` sucht dabei explizit nach den Tags der einzelnen Urkundenabschnitte und übergibt diese dem `TrainingSentence`-Objekt als `truePartLabel` bzw. `trueParagraphLabel`, wohingegen `Diploma` die direkten Textknoten des `tenor`-Tags (Volltext der Urkunde) auf Klassifikationseinheiten hin untersucht und für diese `Sentence`-Objekte erstellt. 

***Hinweis:*** *Dieses Vorgehen ist für die hier benutzten Daten vollkommen ausreichend, birgt aber Probleme: `(Training)Diploma` berücksichtigt nur den Inhalt von `#text`-Kindknoten des Tenors. Das ist ausreichend für die Monasterium-Urkunden, da alle Nicht-Textknoten dort `sup`-Knoten sind, die lediglich Anmerkungen enthalten. CEI bietet aber theoretisch auch die Möglichkeit, z.B. Personen- und Ortsnamen mit einem eigenen Tag zu umschließen. Diese würden dann jedoch weder von `Diploma` noch `TrainingDiploma` ausgewertet werden. Da dieser Fall in den benutzten Trainingsdaten nicht auftaucht und so keine Testbasis bereit liegt, habe ich darauf verzichtet, dafür Vorkehrungen zu treffen. Bei einer möglichen Weiternutzung des Projekts müssten die möglichen Kindknoten des Tenors jedoch stärker berücksichtigt werden.*
***Achtung:*** *Das heißt auch, dass das Klassifizieren bereits annotierter Dateien momentan zur Ausgabe der Ursprungsdatei mit leerem Tenor führt!*

Die Erkennung der Klassifikationsabschnitte selbst ist für die beiden Urkundenklassen identisch, da sie zu diesem Zweck auf die Methode `getSentenceNodes()` in der Superklasse zurückgreifen. Wie genau werden die Klassifikationseinheiten nun erkannt? Die Urkunden liegen in der Form eines Fließtextes vor, der keine äußere Trennung zwischen den einzelnen Urkundenabschnitten vornimmt. Da ganze, bereits zusammengehörige Abschnitte also ausgeschlossen sind und einzelne Urkundenabschnitte auch innerhalb eines Satzes enden oder beginnen können, muss auf Ebene von Teilsätzen nach den Klassifikationseinheiten gesucht werden.

Problematisch ist, dass die Urkunden zusätzlich zum eigentlichen Text der Urkunde in der Regel unannotierte editorische Anmerkungen enthalten, die für ein bestimmtes textuell schwierig abzubildendes Zeichen im Originaldokument stehen. Das sind z. B. das Chrismon als „(C.)“, das Herrschermonogramm als „(M.)“ oder das Rekognitionszeichen „(S. R.)“, alles wahlweise mit oder ohne Klammern, mit oder ohne Punkt. Diese Zeichen sind jedoch semantisch wichtig für die Klassifizierung und sollten darum als eigene Klassifikationsabschnitte erkannt werden.

Aus diesen Überlegungen ergibt sich, dass der Text einer Urkunde zunächst anhand von Satzzeichen sowie umklammerten Anmerkungen in einen Array aufgesplittet werden muss, um die Klassifikationseinheiten zu erhalten. Die so erzeugten Abschnitte können jedoch falsche Grenzen liefern: Auch Abkürzungen enden mit einem Punkt, obwohl sie kein Satzende markieren, und umklammerte Anmerkungen können editorische Informationen enthalten, die keine semantische Bedeutung haben (z.B. „(letztes u im Wort schlecht lesbar)“), ganz zu schweigen von Aneinanderreihungen verschiedener Satzzeichen, die auch als editorische Anmerkung gedacht sein können. Außerdem tauchen in den Urkunden häufig Aufzählungen auf, die durch Kommata getrennt sind. Auch hier wäre es nicht ratsam, jedes einzelne Element der Aufzählung als eigenen Satzteil zu behandeln, da sie meist nur aus einem Wort bestehen.
Die initial erzeugten Elemente des Arrays müssen in einem nächsten Schritt also noch einmal überprüft werden. Je nachdem, ob sie ein tatsächliches Teilsatzende bilden oder nicht, werden sie noch an den letzten Satz angehängt oder als eigener Satz behandelt.

Die Methode `getSentenceNodes()` erstellt weder Satz-Strings noch Satz-Objekte, sondern einzelne Listen mit den Node-Objekten, die zu dem jeweiligen Satz gehören. Der eigentliche Text der Sätze befindet sich in `#text`-Knoten. Neben diesen Knoten können zu einem Satz aber noch weitere Knoten gehören, zum Beispiel das Element `<cei:sup>`, das für Anmerkungen genutzt wird. Es wird immer hinter den jeweiligen Textabschnitt gehängt, hinter dem es in der Ursprungs-XML auftaucht. Dieses Vorgehen ermöglicht es der `Diploma`-Klasse, welche die von der Methode zurückgegebene `SentenceNodesList` als Feldvariable speichert, bei der Erstellung der annotierten Output-XML alle in der Ursprungsdatei enthaltenen Elemente in der richtigen Reihenfolge wieder auszugeben. Aus diesem Grund wird bei der Erzeugung der Klassifikationseinheiten auch auf eine vorherige Normalisierung des Urkundentextes verzichtet, die einige der oben genannten Problematiken vereinfachen, jedoch eine originalgetreue Wiedergabe des Textes erschweren oder unmöglich machen würde.

Die gefundenen und in einzelne zusammengehörige Listen eingeteilten Textknoten werden durch die `Diploma`- bzw. `TrainingDiploma`-Klasse ausgewertet und der Textinhalt eines jeden Satzes sowie das Urkunden-Objekt selbst dem Konstruktor der jeweiligen Instanz von `AbstractSentence` übergeben. Alle Sätze werden außerdem in einer Liste innerhalb des Urkunden-Objekts gespeichert, so dass sowohl vom Satz auf die Ursprungsurkunde als auch von der Urkunde auf alle darin enthaltenen Sätze geschlossen werden kann.

## 3. Präprozessierung
Im Zuge der Erstellung der Klassifikationseinheiten, im Folgenden Sätze genannt, wird der Text der Sätze normalisiert, um ihn für die spätere Klassifikation vorzubereiten. Für die Präprozessierung bietet die Klasse `Preprocessor` die nötigen Methoden. Um die Texte von „Verunreinigungen“ durch nicht-XML-annotierte editorische Anmerkungen zu reinigen, werden zunächst drei CSV-Dateien mit Regex-Ausdrücken und dazugehörigen Ersatz-Ausdrücken angewendet (alle Regex-CSV-Dateien befinden sich unter **config.txts**). Diese müssen dem `Preprocessor` im Konstruktor übergeben werden und werden dort mittels der Methode `ReaderWriter.readCSV()` eingelesen. Es handelt sich dabei um Auflösungen von Klammern (Bsp.: „(C)“ wird zu „chrismon“), Abkürzungen (Bsp.: „recog.“ wird zu „recognovi“) und sonstigen editorischen Anmerkungen (Bsp.: „xxx“ wird zu einem leeren String). Nachdem die entsprechenden Stellen des Textes solchermaßen normalisiert worden sind, folgt die generelle Normalisierung des lateinischen Textes. Im Zuge dessen werden überflüssige Zeichen entfernt, bestimmte lateinische Buchstaben vereinheitlicht (u=v, i=j) und der Text in Kleinbuchstaben übertragen. Außerdem werden die aus Buchstaben bestehenden römischen Zahlen erkannt und durch die Platzhalterzahl „123“ ersetzt. Da im Lateinischen in der Regel nur Satzanfänge und Eigennamen großgeschrieben werden, wurde eine primitive ‚Named Entity Recognition‘ implementiert, bei der alle Wörter, die großgeschrieben sind und innerhalb eines Satzes stehen (also nicht in ihrer Funktion als Satzanfang großgeschrieben sind), durch einen Platzhalternamen ersetzt werden. Gewisse Wörter, die häufig großgeschrieben werden, obwohl sie nicht als Eigenname behandelt werden sollen (z.B. „Deus“, „September“), werden zuvor ebenfalls mittels einer CSV-Datei mit kleinen Anfangsbuchstaben versehen. Diese Vereinheitlichung ermöglicht bessere Klassifikationsergebnisse, die nicht durch die Andersartigkeit verschiedener Eigennamen oder Zahlenstrings beeinträchtigt werden. Nach der Normalisierung werden die Sätze tokenisiert, indem sie anhand der Leerzeichen gesplittet werden.

Der nächste Schritt in der Präprozessierung ist die Lemmatisierung der Satztokens. Diese wird aus Performanzgründen nicht mehr innerhalb der Urkunden-Klassen vorgenommen (was der Funktion der Klasse entsprechend eigentlich am sinnvollsten wäre), da zwecks dessen ein externes Programm angesprochen werden muss, welches eine relativ lange Zeit zum Laden der eigenen Datenbanken benötigt. Aus diesem Grund werden zunächst alle erstellten Satzobjekte in der `DiplomaAnalyzerApp` gesammelt und diese erst bei der Übergabe an den Konstruktor des `ProbabilisticClassifier` (im Falle von `TrainingSentence`) bzw. dessen `classify()`-Methode (`AbstractSentence`-Objekte, die klassifiziert werden sollen) lemmatisiert, indem von dort aus die Methode `lemmaTokenizeSentences()` des Preprocessor aufgerufen wird.

Zur Lemmatisierung wird mit dem externen Lemmatizer [LEMLAT 3.0](http://www.lemlat3.eu/) gearbeitet. LEMLAT arbeitet wörterbuchbasiert und bietet für das hiesige Vorhaben den Vorteil, dass die internen Datenbanken nicht nur klassische lateinische Wörterbücher enthalten, sondern auch das Mittellatein-Wörterbuch Glossarium Mediae et Infimae Latinitatis von Du Cange. Leider ist LEMLAT keine Library, die per Maven-POM eingebunden werden kann, sondern es handelt sich um eine eigene Anwendung, die über die Konsole bedient werden muss.

***Hinweis:*** *Die Anwendung sowie alle dazugehörigen Datenbanken müssen sich im Ordner **lemlat** befinden.*
***Achtung:*** *Die Kommunikation zwischen dem `DiplomaAnalyzer` und LEMLAT wurde auf dem Betriebssystem Windows entwickelt. Diese Plattformabhängigkeit kann dazu führen, dass die Lemmatisierung beim Ausführen der `DiplomaAnalyzerApp` auf einem anderen Betriebssystem bzw. mit einer anderen Instanz von LEMLAT keine Ergebnisse liefert. In einem solchen Fall sollte das Programm eine `Exception` abfangen und mit den unlemmatisierten Tokens weiterrechnen.*

Die Kommunikation zwischen dem DiplomaAnalyzer-Programm und der **lemlat.exe** stellt die Methode `getPossibleLemmasForTypes()` her. Sie führt die **lemlat.exe** über die `Runtime.getRuntime().exec()` als `Process`-Objekt aus und kommuniziert mit ihr über den `InputStream` bzw. `OutputStream` des Prozesses. Dem zum Prozess führenden `OutputStream` wird pro Zeile jeweils eines der zu lemmatisierenden Token übergeben. LEMLAT gibt pro Token nicht nur die möglichen Lemmata (keines, eins oder mehrere), sondern auch detaillierte Analysen zu Deklination/Konjugation aus. Um die Lemmata aus diesem Output herauszufiltern, werden Deklaratoren genutzt, d.h. Strings, die im LEMLAT-Output stets das Auftreten eines neuen Lemmas oder einer Ursprungswortform markieren. Der vom Prozess kommende `InputStream` wird Zeile für Zeile auf diese Deklaratoren überprüft und pro Token werden so die möglichen Lemmata gefunden. Die Tokens werden gemeinsam mit einer Liste ihrer jeweils möglichen Lemmata in einer Map gespeichert und zurückgegeben.

Aus der Liste aller möglichen Lemmata für ein Token muss vor der Zuordnung der Satztokens zunächst ein Lemma bestimmt werden, welches benutzt werden soll. Da das DiplomaAnalyzer-Programm kein POS-Tagging oder semantische Analysen unterstützt, welche eine begründete Entscheidung für ein Lemma ermöglichen könnten, wurde ein naiver Ansatz gewählt, bei dem für ein Token stets dasjenige Lemma ausgewählt wird, welches am häufigsten in der jeweiligen Liste der möglichen Lemmata auftritt. Bei gleicher Häufigkeit wird das zuerst auftauchende Lemma gewählt.

Da der gesamte Vorgang der Lemmatisierung mit LEMLAT viel Zeit in Anspruch nimmt, ist es ratsam, kein Token zweimal an die LEMLAT-Anwendung zu übergeben. Die bereits lemmatisierten Tokens und ihr jeweiliges „bestes“ Lemma werden darum in der `lemmaPairs`-Map des `Preprocessor` gespeichert (durch `updateLemmaPairs()`). Bei jedem Aufruf von `lemmaTokenizeSentences()` wird dann zunächst überprüft, welche der zu lemmatisierenden Satztokens sich noch nicht in `lemmaPairs` befinden. Nur diese noch nicht lemmatisierten Tokens werden zur Lemmatisierung mit LEMLAT weitergereicht, was eine enorme Zeitersparnis bedeutet. Sobald `lemmaPairs` alle für die im aktuellen Aufruf übergebene Satz-Liste benötigten Satztokens enthält, wird anhand der `tokens`-Liste eines jeden Satzobjektes eine `lemmatizedTokens`-Liste erstellt und im Objekt gespeichert. Ein letzter Schritt der Präprozessierung ist die Vektorisierung der Sätze zwecks probabilistischer Klassifikation. Auf diese wird im nächsten Kapitel genauer eingegangen.

## 4. Klassifikation
Ziel des Programms ist es, jeden Satz einem passenden Urkundenabschnitt zuzuweisen. Die Urkundenabschnitte werden repräsentiert durch die Enums `DiplomaticLabel` und `DiplomaticParagraphLabel` im Package **dataClasses.label**. Dabei enthält `DiplomaticParagraphLabel` die Label Protokoll, Kontext und Eschatokoll, die die drei großen Abschnitte einer Urkunde bezeichnen. `DiplomaticLabel` enthält die Bezeichnungen aller Unterabschnitte. Bei der Klassifikation werden nur die `DiplomaticLabel` berücksichtigt, da sich durch diese automatisch die richtige Zuweisung der `DiplomaticParagraphLabel` ergibt.

Für die Klassifikation werden zwei unterschiedliche Ansätze kombiniert, nämlich ein regelbasierter und ein probabilistischer. Ersteren repräsentiert die Klasse `DiplomaticClassifier`, die die Label-Zuweisung gemäß Regeln der Diplomatik vornimmt. Der zweite Ansatz entspricht der Klasse `ProbabilisticClassifier`, in der eine Klassifikation mittels Machine Learning erfolgt. Beide Klassen arbeiten wesentlich mit einem Member der Satz-Objekte, nämlich den `labelProbs`. Dabei handelt es sich um ein Array, das Wahrscheinlichkeitswerte für jedes Label enthält. Auf Basis dieser Wahrscheinlichkeiten kann jedem Satz später das höchst bewertete Label zugewiesen werden.

Bei der Erstellung eines Satzobjektes wird die Wahrscheinlichkeit für jedes Label automatisch mit 1 initialisiert. Bereits durch den Aufruf der Funktion `assignSequenceBasedProbabilities()` jedoch werden die Wahrscheinlichkeiten differenziert. Diese Methode aus der `DiplomaticClassifier`-Klasse greift auf ein Objekt der Klasse `Milestones` (Package **config**) zurück, welches ihr im Konstruktor übergeben wird. Die `Milestones`-Klasse ist eine Datenklasse, die ihre Daten anhand der im Konstruktor übergebenen Trainingsurkunden selbst berechnet. Sie enthält Angaben über die durchschnittliche absolute Länge der Oberabschnitte Protokoll, Kontext und Eschatokoll. Genauer gesagt enthält sie das durchschnittliche letzte Wort des Protokolls und das durchschnittliche erste Wort des Eschatokolls. Letzterer Wert ist dabei invertiert, d.h. relativ zum Ende der Urkunde, da die Länge des Kontextes im Gegensatz zu demjenigen von Eschatokoll und Protokoll stark variieren kann und der nicht-invertierte Eschatokollstart darum an Aussagekraft einbüßt. Außerdem berechnet `Milestones` die durchschnittlichen Labelwahrscheinlichkeiten, das heißt die Wahrscheinlichkeit mit der ein zufällig ausgewählter Satz zu einem bestimmten Label gehört, basierend auf der relativen Länge der einzelnen Label-Abschnitte (Unterabschnitte) in den Trainingsurkunden.

Die durchschnittlichen Labelwahrscheinlichkeiten führen zu einer initialen Höhergewichtung häufig und umfangreich auftauchender Urkundenabschnitte, was eine Einordnung des Satzes auch dann ermöglicht, wenn weder eine regelbasierte noch probabilistische Klassifikation zu starken Ergebnissen führt, zum Beispiel wegen fehlender aussagekräftiger Tokens. Da die mögliche Reihenfolge der einzelnen Label jedoch strikt vorgegeben ist, kann auf Grundlage der Position des Satzes innerhalb der Urkunde sogar eine noch genauere Wahrscheinlichkeitszuordnung vorgenommen werden. Zwecks dessen werden für die drei Oberabschnitte Protokoll, Kontext und Eschatokoll präzisere Wahrscheinlichkeiten ermittelt, indem die durch- schnittliche Länge ihrer Abschnitte nicht relativ zur Gesamtlänge der Urkunde, sondern relativ zur Länge des Oberabschnitts berechnet wird. Um die Differenz zwischen dem solchermaßen höher gewichteten Abschnitt und den benachbarten Labeln nicht zu extrem werden zu lassen, kann im Konstruktor von `Milestones` ein Toleranzwert übergeben werden, der angibt, wie viele Label vor und nach dem jeweiligen Absatz ebenfalls eine Höhergewichtung erfahren sollen. Deren Wahrscheinlichkeit wird dann relativ zur Länge des Oberabschnitts plus der Länge aller Toleranzabschnitte berechnet. Alle Label, die nicht zum Abschnitt oder zu den Toleranzlabeln gehören, erhalten die jeweilige zuvor berechnete allgemeine Labelwahrscheinlichkeit.

Die Funktion `assignSequenceBasedProbabilities()` des `DiplomaticClassifiers` überprüft anhand des durchschnittlichen Protokollendes und Eschatokollstarts, zu welchem Oberabschnitt ein jeder übergebener Satz gehört und weist dem Satz die entsprechenden höhergewichteten Labelwahrscheinlichkeiten zu, so dass für die Sätze bereits vor der inhaltlichen Klassifikation bestimmte Label wahrscheinlicher sind als andere. Die längen- und positionsabhängigen Labelwahrscheinlichkeiten werden im nächsten Schritt mit den Wahrscheinlichkeiten verrechnet, die die Klassifikation durch das Machine Learning ergibt.

Diese Klassifikation wird in der Klasse `ProbabilisticClassifier` vorgenommen. Die Klasse verwaltet die Klassifikation mittels des Naive Bayes-Classifiers der [Weka-Library](https://www.cs.waikato.ac.nz/ml/weka/index.html) der Waikato Universität in Neuseeland, die als Maven-Dependency angegeben ist. Wie bereits erwähnt, werden alle Sätze vor der Klassifizierung lemmatisiert. Durch die Parameter `useBigramsInsteadOfTokens` und `vectorType` im Konstruktor des `ProbabilisticClassifier` kann außerdem ausgewählt werden, ob mit Bigrammen gearbeitet und welche Vektorart verwendet werden soll. Die Vektorisierung geschieht im `Preprocessor` und kann binär, häufigkeitsbasiert oder Tf-Idf-basiert geschehen. Nachdem alle Sätze lemmatisiert, gegebenenfalls bigrammisiert, vektorisiert sowie in ein vom Weka-Classifier benutztes Objekt der Klasse `Instance` übertragen worden sind, werden die Sätze per Naive Bayes klassifiziert. Die Klassifikation mit Naive Bayes arbeitet bereits genuin nicht mit einer festen Label-Zuweisung, sondern berechnet für jedes mögliche Label eine Wahrscheinlichkeit. Statt dem Satz standardmäßig dasjenige Label mit der höchsten Wahrscheinlichkeit zuzuweisen, werden nun alle Wahrscheinlichkeiten mit den im Satz bereits vorhandenen Labelwahrscheinlichkeiten verrechnet, so dass bereits hier der regelbasierte mit dem probabilistischen Ansatz verknüpft wird.

Nach diesen Berechnungen wird vor der abschließenden Zuweisung der Label auf Basis von Wahrscheinlichkeitswerten durch die Methode `assignByIndicators()` der Klasse `DiplomaticClassifier` zunächst eine direkte Label-Zuweisung basierend auf vorgegebenen Indikatoren durchgeführt. Diese zielt darauf ab, möglichst viele solcher Sätze bereits zu klassifizieren, welche durch bestimmte Formeln eindeutig zuordbar sind, und so bereits eine grobe Strukturierung der Urkunde vorzunehmen, die später für die abschließende Reihenfolge-konforme Label-Zuweisung hilfreich ist. Die Methode `assignByIndicators()` arbeitet mit einer CSV-Datei, die pro Zeile genau vier Werte enthält:

1.	**Der Indikator.** Hierbei handelt es sich um einen String, dessen Vorkommen innerhalb eines Satzes auf ein bestimmtes Label hinweist. Die Indikatoren können ein oder mehrere Wörter lang sein. Sie sollten so gewählt sein, dass sie möglichst charakteristisch für das mit ihnen verbundene Label sind, so dass Fehlzuweisungen ausgeschlossen sind. Precision ist hier wichtiger als Recall, da die Label bei einer Übereinstimmung zwischen Satz und Indikator ohne jegliche weitere Überprüfung zugewiesen werden. Die Indikator-Strings werden vor dem Ab- gleich durch die Methode `normalizeLatinText()` des `Preprocessor` normalisiert.
2.	**Der Name des Labels**, auf das der Indikator hinweist.
3.	**Ähnlichkeitsgrad**. Hierbei handelt es sich um eine Kennziffer, die angibt, welcher Ähnlichkeitsgrad zwischen dem Indikator und dem Satz vorliegen muss, damit die Label-Zuweisung greift. Je nach gewähltem Grad werden unterschiedliche Stringmatching-Methoden der Klasse `SimilarityCalculator` eingesetzt. Um die mittelalterliche Tendenz zu variierenden Schreibweisen eigentlich identischer Wörter abzufangen, wird dabei nie auf Charakter-Ebene verglichen, sondern nur auf Token-Ebene. Ob zwei Tokens ähnlich sind, wird dabei durch die Berechnung der Needleman-Wunsch-Ähnlichkeit entschieden (die Methode zur Berechnung der Needleman-Wunsch-Ähnlichkeit habe ich aus dem Forschungsprojekt [Qualifikationsentwicklungsforschung](https://github.com/spinfo/quenfo/blob/master/src/main/java/quenfo/de/uni_koeln/spinfo/categorization/workflow/SimilarityCalculator.java) des Instituts für Digital Humanities der Universität zu Köln entnommen). Die möglichen Ähnlichkeitsgrade und die dazugehörigen Stringmatching-Methoden sind:
    * 1 = Substring:* Der Indikatorsatz ist ein Substring des Originalsatzes, d.h. der Originalsatz enthält alle Wörter des Indikator-Strings in der richtigen Reihenfolge und ohne Unterbrechungen. Die benutzte Matching-Methode ist `containsSimilarTokenSubstring()`.
    * *2 = Subset:* Der Indikatorsatz ist ein Subset des Originalsatzes, d.h. der Originalsatz enthält alle Wörter des Indikator-Strings. Die benutzte Matching-Methode ist `containsAllTokens()`.
    * *3 = Similar (Default):* Der Indikatorsatz ähnelt einem Substring des Originalsatzes. Als ähnlich gelten die beiden (Sub)Strings, wenn ihr Jaccard-Koeffizient größer/gleich 0.8 ist. Die benutzte Matching-Methode ist `similarityOfSubstring()`.
4.	**Satzposition.** Hierbei handelt es sich um eine Kennziffer, die angibt welche ungefähre Position des Satzes innerhalb der Urkunde vorliegen muss, damit die Label-Zuweisung greift. Die Ziffern 1-3 stehen für die drei großen Abschnitte Protokoll, Kontext und Eschatokoll. Zu welchem Abschnitt der Satz vermutlich gehört, wird bestimmt anhand seines Wortindexes sowie des durchschnittlichen Protokollendes und Eschatokollstarts aus dem `Milestones`-Objekt.

Für jeden Satz und jeden Indikator werden die genannten Bedingungen geprüft und bei Übereinstimmung dem Satz das entsprechende Label zugewiesen. Nach Terminierung der Funktion haben manche Sätze also bereits Label, einige jedoch noch nicht. Bevor allen bisher ungelabelten Sätzen ein Label zugewiesen werden kann, wird erst die Methode `fillSequence()` des `DiplomaticClassifier` aufgerufen, die ungelabelten Sätzen, die zwischen zwei Sätzen desselben Labels stehen, das Label dieser Nachbarsätze zuweisen. Erst dann wird mit `assignByDefault()` die abschließende Klassifikation aller Sätze vorgenommen. Hierfür ist zunächst ausschlaggebend, welche Label der letzte bereits gelabelte Satz und der nächste gelabelte Satz haben. Da die Urkundenabschnitte eine bestimmte Reihenfolge einhalten müssen, kann der Satz nur ein Label erhalten, welches zwischen den bereits vergebenen Labeln liegt oder ihnen entspricht. Aus diesen möglichen Labeln wird nun jenes ausgewählt und zugewiesen, welches die höchste Labelwahrscheinlichkeit besitzt. Dieses Label beeinflusst zugleich die möglichen Label aller nachfolgenden Sätze.

Nachdem auf diese Weise alle Sätze ein Label erhalten haben und ihnen schlussendlich auf Basis dieser Label noch jeweils ein Label für den dazugehörigen Oberabschnitt zugewiesen worden ist, wird für jede Klassifikations-Urkunde auf Basis der beim Einlesen der Quelldatei angelegten `sentenceNodes` eine Output-XML erstellt, in der der Ursprungstext nun durch die richtigen CEI-Tags für die Urkundenabschnitte ergänzt wird. Diese Dateien werden im Ordner **data/outputXMLdocs** gespeichert

## 5.  Evaluation

Die `DiplomaAnalyzerApp` ermöglicht es, zu Beginn die Option Evaluation zu wählen. Dadurch wird die Methode `evaluate()` der Klasse Evaluation aufgerufen. In dieser Klasse wird eine Kreuzvalidierung auf Basis aller Trainingsdaten vorgenommen. Um verschiedene Konfigurationsszenarien durchspielen zu können, arbeitet die Klasse mit einer Liste an Objekten der Klasse `ClassificationConfig` (Package **config**). Mithilfe dieser Klasse kann bestimmt werden, ob Bigramme oder Tokens verwendet werden sollen, welcher Vektorisierungstyp genutzt wird und mit welcher Toleranz die abschnittsabhängigen Labelwahrscheinlichkeiten berechnet werden sollen. Ein Objekt dieser Klasse wird auch für die normale Klassifikation verwendet, allerdings wird dort der Default-Konstruktor aufgerufen.

In der Evaluationsklasse werden 18 verschiedene Konfigurationen erstellt und getestet. Pro Konfiguration wird nach der Kreuzvalidierung eine Textdatei mit der jeweiligen Konfusionsmatrix und den berechneten Evaluationsmaßen erstellt. Diese Dateien werden unter **documentation/evaluationResults** gespeichert.

***Hinweis:*** *Die Evaluationsergebnisse können im Einzelfall von den tatsächlichen Ergebnissen der Output-Dateien abweichen. Das liegt daran, dass in den Testdaten manchmal zwischen zwei Abschnitten kein Punkt oder sonstiges Trennzeichen gesetzt wird und das Programm dann zwei Teilsätze als einen Satz auffasst und klassifiziert. Da die Evaluation mit den Trainingsdaten arbeitet, in denen die Abschnitte unabhängig von Satzzeichen annotiert werden, fällt dieser Einflussfaktor hier heraus.*

Die Evaluationsergebnisse (siehe die Übersichtstabelle in Abschn. 6) zeigen, dass die Ergebnisse der Vektorisierungsarten binary und count sehr ähnlich sind, wohingegen die Ergebnisse des Tf-Idf-Vektors in allen Werten nicht wesentlich aber teils doch deutlich niedriger liegen. Das könnte dem Umstand geschuldet sein, dass aufgrund der Formelhaftigkeit der Urkundenabschnitte oft auch häufig vorkommende Wörter eine große Bedeutung haben, die durch die Tf-Idf-Berechnung jedoch verringert wird. Dass binary und count sich sehr ähneln, liegt mit an der Kürze der Klassifikationseinheiten, in denen einzelne Wörter ohnehin meist nur einmal pro Einheit vorkommen.

Weiterhin fällt auf, dass eine Toleranz von 0 bei der Berechnung der abschnittsabhängigen Labelwahrscheinlichkeiten etwas schlechtere Ergebnisse liefert als eine Toleranz von 1 oder 2. Die besten Werte liefert hierbei 1. Besonders interessant ist, dass die Verwendung von Bigrammen für die Vektorarten binary und count unabhängig vom Toleranzwert in allen Evaluationswerten deutlich bessere Ergebnisse liefert als die Verwendung einfacher Tokens. Auch das ist vermutlich der Formelhaftigkeit der Urkunden geschuldet. Die einzelnen Standard-Ausdrücke können durch Bigramme besser erkannt werden als durch Tokens. Die beste der hier getesteten Konfigurationen ist Vektorisierungsart = binary, Toleranz = 1, Bigramme = true.

Insgesamt lässt sich bei der Betrachtung der `evaluationResults`-Dateien erkennen, dass einzelne Label teils extrem hohe Evaluationswerte im Bereich der oberen 5 Prozent aufweisen, nämlich vor allem die häufig sehr ähnlich formulierten Abschnitte des Protokolls und des Eschatokolls. Hier fällt lediglich die Inscriptio aus dem Rahmen, die Adressierung des Empfängers der Urkunde. Diese ist meist auch sehr formelhaft, taucht in den Trainingsdaten jedoch kaum auf, so dass sie nicht erkannt wird. Gleiches gilt für die Sanctio.

Besonders zwischen der Narratio, Dispositio und Corroboratio scheinen hingegen die Grenzen zu verwischen. Hier sind sowohl Recall als auch Precision teils sehr schlecht. Das korreliert mit einer generellen Schwierigkeit, diese Abschnitte einzuteilen – auch für Menschen. Denn häufig enthalten zum Beispiel Narratio und Dispositio ähnliche Inhalte, die nur durch wenige Schlüsselwörter getrennt sind. Da diese Wörter jeglicher Natur sein können und eben nicht zwangsläufig nur in der Dispositio auftauchen müssen, können sie nicht als Indikatoren genutzt werden. Die schlechteren Klassifikationsergebnisse des Programms sind in diesem Be- reich also zwar bedauerlich aber lediglich einem natürlichen Umstand geschuldet, der sich in den Trainingsdaten niederschlägt. 

## 6. Anhang: Tabelle Evaluationsergebnisse
Stand: August 2019; Format der Konfiguration: Vectortyp, Toleranz, Bigramme

1. Prec, Rec, Acc und F1 der Macro-Averages (nur Nachkst.)
2. (Rec, Prec, F1) und Acc der Micro-Averages (nur Nachkst.)

| Konfiguration    | Prec | Rec  | Acc  | F1   | Rec, Prec, F1 | Acc  |
| ---------------- | ---- | ---- | ---- | ---- | ------------- | ---- |
| binär, 0, false  | 8006 | 6942 | 9440 | 7006 |     6467      | 9440 |
| binär, 0, true   | 8106 | 7062 | 9480 | 7138 |     6720      | 9480 |
| binär, 1, false  | 8026 | 6978 | 9447 | 7043 |     6512      | 9447 |
| binär, 1, true   | 8129 | 7088 | 9483 | 7172 |     6736      | 9483 |
| binär, 2, false  | 8062 | 6978 | 9447 | 7043 |     6512      | 9447 |
| binär, 2, true   | 8105 | 7073 | 9481 | 7149 |     6726      | 9481 |
| count, 0, false  | 8006 | 6938 | 9437 | 7001 |     6445      | 9437 |
| count, 0, true   | 8070 | 7048 | 9472 | 7114 |     6669      | 9472 |
| count, 1, false  | 8026 | 6976 | 9444 | 7037 |     6493      | 9444 |
| count, 1, true   | 8118 | 7079 | 9476 | 7160 |     6691      | 9476 |
| count, 2, false  | 8026 | 6976 | 9444 | 7037 |     6493      | 9444 |
| count, 2, true   | 8093 | 7064 | 9474 | 7138 |     6682      | 9474 |
| Tf-Idf, 0, false | 7860 | 7029 | 9340 | 6937 |     5838      | 9340 |
| Tf-Idf, 0, true  | 7696 | 6877 | 9322 | 6758 |     5723      | 9322 |
| Tf-Idf, 1, false | 7867 | 7054 | 9344 | 6956 |     5860      | 9344 |
| Tf-Idf, 1, true  | 7704 | 6882 | 9322 | 6766 |     5726      | 9322 |
| Tf-Idf, 2, false | 7867 | 7054 | 9344 | 6956 |     5860      | 9344 |
| Tf-Idf, 2, true  | 7704 | 6882 | 9322 | 6766 |     5726      | 9322 |
